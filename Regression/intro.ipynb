{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "\n",
    "y = b0 + b1.x + ε OR \\\n",
    "Y = β0. + β1 x + ε \\\n",
    "y -> dependent variable \\\n",
    "x -> independent variable \\\n",
    "b0 -> intercept coefficient \\\n",
    "b1 -> slope coefficient \\\n",
    "ε -> error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares:\n",
    "\n",
    "We have our data point in the graph. Now the question is which of the slope line is the best one. For that we can use 'Ordinary Least Squares' method.\n",
    "\n",
    "<!-- ![OLS](../../Data%20Images/OLS.png) -->\n",
    "\n",
    "<img src=\"../Data%20Images/OLS.png\" alt=\"Project Setup\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions:\n",
    "<img src=\"../Data%20Images/Linear Assumption.png\" alt=\"Project Setup\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables and Avoiding the Dummy Variable Trap\n",
    "\n",
    "When dealing with categorical data in machine learning, we often need to convert these categories into numerical values that algorithms can understand. One common method is to use **dummy variables**.\n",
    "\n",
    "### What are Dummy Variables?\n",
    "\n",
    "Dummy variables, also known as indicator variables, are used to represent categorical data as binary (0 or 1) variables. Each category becomes a separate variable.\n",
    "\n",
    "### Example: Converting Categorical Data\n",
    "\n",
    "Suppose you have a column with the names of cities: `New York`, `Los Angeles`, and `Chicago`.\n",
    "\n",
    "```plaintext\n",
    "City\n",
    "New York\n",
    "Los Angeles\n",
    "Chicago\n",
    "New York\n",
    "Chicago\n",
    "Los Angeles\n",
    "```\n",
    "\n",
    "### Creating Dummy Variables\n",
    "\n",
    "For this column, you would create three new binary variables:\n",
    "\n",
    "1. `New York`\n",
    "2. `Los Angeles`\n",
    "3. `Chicago`\n",
    "\n",
    "Each row would have a 1 in the column corresponding to its city and 0s in the others.\n",
    "\n",
    "```plaintext\n",
    "City         New York  Los Angeles  Chicago\n",
    "New York     1         0            0\n",
    "Los Angeles  0         1            0\n",
    "Chicago      0         0            1\n",
    "New York     1         0            0\n",
    "Chicago      0         0            1\n",
    "Los Angeles  0         1            0\n",
    "```\n",
    "\n",
    "### Avoiding the Dummy Variable Trap\n",
    "\n",
    "However, using all these dummy variables in your model can cause multicollinearity issues, known as the **dummy variable trap**. This happens because one of the dummy variables can be perfectly predicted from the others, which can lead to redundant information and issues with model estimation.\n",
    "\n",
    "To avoid the dummy variable trap, you should use \\( k-1 \\) dummy variables for \\( k \\) categories. In our example with three categories, you would use only two dummy variables.\n",
    "\n",
    "### Example: Using \\( k-1 \\) Dummy Variables\n",
    "\n",
    "Let's choose to drop the dummy variable for `Chicago`. This means we will use only `New York` and `Los Angeles`:\n",
    "\n",
    "```plaintext\n",
    "City         New York  Los Angeles\n",
    "New York     1         0\n",
    "Los Angeles  0         1\n",
    "Chicago      0         0\n",
    "New York     1         0\n",
    "Chicago      0         0\n",
    "Los Angeles  0         1\n",
    "```\n",
    "\n",
    "In this setup:\n",
    "- `New York` is 1 if the city is New York, 0 otherwise.\n",
    "- `Los Angeles` is 1 if the city is Los Angeles, 0 otherwise.\n",
    "- If both `New York` and `Los Angeles` are 0, the city must be Chicago.\n",
    "\n",
    "### Implementation in Python\n",
    "\n",
    "Here's how you can create dummy variables in Python using pandas:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Chicago', 'Los Angeles']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create dummy variables\n",
    "dummies = pd.get_dummies(df['City'], drop_first=True)\n",
    "\n",
    "# Concatenate the original DataFrame and the dummy variables\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "\n",
    "```plaintext\n",
    "          City  Los Angeles  New York\n",
    "0     New York            0         1\n",
    "1  Los Angeles            1         0\n",
    "2      Chicago            0         0\n",
    "3     New York            0         1\n",
    "4      Chicago            0         0\n",
    "5  Los Angeles            1         0\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- The `City` column is transformed into dummy variables.\n",
    "- We use the `drop_first=True` parameter in `pd.get_dummies()` to avoid the dummy variable trap by dropping the first category (Chicago in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Model\n",
    "In Multiple Linear Regression, we have to choose which independent variable we have to keep and which to throw out. Here are Some methods:\n",
    "- All in\n",
    "- Backward Elimination\n",
    "- Forward Selection\n",
    "- BiDirectional Elimination\n",
    "- Score \n",
    "\n",
    "### Backward Elimination\n",
    "Backward Elimination is a statistical method used to select the most significant features in a regression model. This process involves iteratively removing the least significant feature based on a chosen significance level (p-value) until only statistically significant features remain.\n",
    "\n",
    "Here's a step-by-step explanation of the Backward Elimination process:\n",
    "\n",
    "1. **Select a significance level** (e.g., SL = 0.05).\n",
    "2. **Fit the full model** including all possible predictors.\n",
    "3. **Consider the predictor with the highest p-value**. If the p-value is greater than the significance level, remove this predictor.\n",
    "4. **Refit the model** without the removed predictor(without removed variable).\n",
    "5. **Repeat steps 3-4** until all remaining predictors have p-values less than the significance level.\n",
    "\n",
    "\n",
    "### Forward Selection\n",
    " is a stepwise approach to model selection where we start with an empty model and iteratively add features that improve the model the most. This process continues until no significant improvement can be made by adding any remaining feature.\n",
    "\n",
    "Here's how you can implement Forward Selection in Python using the given dataset:\n",
    "\n",
    "### Steps for Forward Selection\n",
    "\n",
    "1. **Start with an empty model**.\n",
    "2. **Add features one by one** based on a chosen significance level (p-value).\n",
    "3. **At each step, add the feature that improves the model the most** (i.e., the one with the lowest p-value that is below the significance level).\n",
    "4. **Stop when no further improvement is possible** (i.e., all remaining features have p-values above the significance level).\n",
    "\n",
    "\n",
    "Bidirectional Elimination is a combination of both Forward Selection and Backward Elimination. In this method, we start by including features step-by-step (like in Forward Selection), but after adding each new feature, we also check if any of the already included features have become insignificant and should be removed (like in Backward Elimination). This ensures that the model includes only the most significant features at each step.\n",
    "\n",
    "Here is how to implement Bidirectional Elimination in Python using the given dataset:\n",
    "\n",
    "### Steps for Bidirectional Elimination\n",
    "\n",
    "1. **Start with an empty model**.\n",
    "2. **Add features one by one** based on a chosen significance level (p-value).\n",
    "3. **At each step of adding a feature, check all features in the model** and remove those that are not significant.\n",
    "4. **Repeat the process** until no more features can be added or removed.\n",
    "\n",
    "\n",
    "**Bidirectional Elimination**:\n",
    "- **Forward Selection Step**: Add features one by one, checking which feature improves the model the most (lowest p-value).\n",
    "- **Backward Elimination Step**: After adding a feature, check all features already in the model and remove those that are not significant.\n",
    "- Repeat the process until no more features can be added or removed that significantly improve the model.\n",
    "\n",
    "### All Possible Models\n",
    "All Possible Models\" (also known as \"Exhaustive Search\") is a method where we evaluate all possible combinations of features to identify the best model based on a chosen criterion (e.g., Adjusted R-squared, AIC, BIC). This method is computationally expensive but guarantees finding the best subset of features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Multiple Linear Regression do not require feature scaling. Coefficient will take of high low values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In Multple Linear Regression, Skilearn.linearmodel->LinearRegression()\n",
    " Will take care of dummy variable trap and Backward elimination. We dont need to explicitly do it by ourself."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
